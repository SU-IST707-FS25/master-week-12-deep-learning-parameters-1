{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c8a76a",
   "metadata": {},
   "source": [
    "# Exercise 1: Custom Activation Functions and Their Impact\n",
    "\n",
    "In this exercise, you'll implement custom activation functions and compare their performance with standard activations. You'll use the Wine Quality dataset, which is small but provides an interesting regression problem.\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1817bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Load Wine Quality dataset\n",
    "wine_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "X = wine_data.drop('quality', axis=1).values\n",
    "y = wine_data['quality'].values\n",
    "\n",
    "# Split and scale the data\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Utility function for plotting\n",
    "def plot_activation_functions(activation_functions, x_range=(-5, 5)):\n",
    "    x = np.linspace(x_range[0], x_range[1], 200)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    for name, fn in activation_functions.items():\n",
    "        y = fn(x)\n",
    "        plt.plot(x, y, label=name)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title(\"Activation Functions\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ceb1f",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Implementing Custom Activation Functions\n",
    "\n",
    "Implement the following custom activation functions:\n",
    "\n",
    "1. Mish: f(x) = x * tanh(softplus(x))\n",
    "2. Swish: f(x) = x * sigmoid(x)\n",
    "3. A custom variant of your choice (be creative!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a77b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_mish(x):\n",
    "    # TODO: Implement the Mish activation function\n",
    "    # Hint: Use tf.math.softplus and tf.math.tanh\n",
    "    return None\n",
    "\n",
    "def custom_swish(x):\n",
    "    # TODO: Implement the Swish activation function\n",
    "    # Hint: Use tf.math.sigmoid\n",
    "    return None\n",
    "\n",
    "def custom_variant(x):\n",
    "    # TODO: Implement your own activation function variant\n",
    "    # Be creative! Consider combining existing functions or creating something new\n",
    "    return None\n",
    "\n",
    "# Test your implementations\n",
    "activation_functions = {\n",
    "    \"Mish\": custom_mish,\n",
    "    \"Swish\": custom_swish,\n",
    "    \"Custom Variant\": custom_variant\n",
    "}\n",
    "\n",
    "# Create test input\n",
    "test_input = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"Test outputs:\")\n",
    "for name, fn in activation_functions.items():\n",
    "    print(f\"{name}: {fn(test_input).numpy()}\")\n",
    "\n",
    "# Plot the activation functions\n",
    "plot_activation_functions(activation_functions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2116f0d",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Creating Models with Custom Activations\n",
    "\n",
    "Create a function that builds a model using a given activation function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d886c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(activation_fn, input_shape=[11]):\n",
    "    # TODO: Implement a model with:\n",
    "    # - 3 Dense layers (64, 32, 1 neurons)\n",
    "    # - Custom activation for hidden layers\n",
    "    # - No activation for output layer\n",
    "    # - He initialization for weights\n",
    "    return None\n",
    "\n",
    "# Create models with different activations\n",
    "activations_to_test = {\n",
    "    \"ReLU\": tf.nn.relu,\n",
    "    \"Mish\": custom_mish,\n",
    "    \"Swish\": custom_swish,\n",
    "    \"Custom\": custom_variant\n",
    "}\n",
    "\n",
    "models = {name: create_model(fn) for name, fn in activations_to_test.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e0d01",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Training and Comparison\n",
    "\n",
    "Train each model and compare their performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3606891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(model, name):\n",
    "    # TODO: Implement training and evaluation\n",
    "    # - Compile model with appropriate loss and metrics\n",
    "    # - Train for 20 epochs\n",
    "    # - Record training time and history\n",
    "    # - Evaluate on test set\n",
    "    # Return training time, history, and test score\n",
    "    return None\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining model with {name} activation:\")\n",
    "    results[name] = train_and_evaluate(model, name)\n",
    "\n",
    "# TODO: Create a comparison DataFrame with:\n",
    "# - Training time\n",
    "# - Final training loss\n",
    "# - Final validation loss\n",
    "# - Test score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91aa6ec",
   "metadata": {},
   "source": [
    "\n",
    "## Part 4: Visualization and Analysis\n",
    "\n",
    "Create visualizations to compare the performance of different activation functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac329d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_training_curves(results):\n",
    "    # TODO: Create plots comparing:\n",
    "    # - Training loss over time\n",
    "    # - Validation loss over time\n",
    "    # - Training vs validation loss for each activation\n",
    "    pass\n",
    "\n",
    "# Create visualizations\n",
    "plot_training_curves(results)\n",
    "\n",
    "# TODO: Calculate and display additional metrics like:\n",
    "# - Training speed (examples/second)\n",
    "# - Number of parameters\n",
    "# - Memory usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232358cd",
   "metadata": {},
   "source": [
    "\n",
    "## Part 5: Analysis Questions\n",
    "\n",
    "1. Which activation function performed best in terms of:\n",
    "   - Final model accuracy?\n",
    "   - Training speed?\n",
    "   - Convergence stability?\n",
    "\n",
    "2. Why do you think your custom activation function performed the way it did?\n",
    "\n",
    "3. What are the tradeoffs between the different activation functions you tested?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f0f11",
   "metadata": {},
   "source": [
    "# Exercise 2: Momentum and Learning Rate Interaction Study\n",
    "\n",
    "In this exercise, we'll explore how momentum and learning rate interact during training. We'll create a systematic study of different combinations and visualize their effects on model training.\n",
    "\n",
    "## Setup and Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43f6ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess Fashion MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Take a subset of data for faster experimentation\n",
    "n_samples = 10000\n",
    "X_train = X_train[:n_samples]\n",
    "y_train = y_train[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfd3fd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 1: Training Infrastructure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "59f2e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seed=42):\n",
    "    \"\"\"Creates a simple neural network\"\"\"\n",
    "    tf.random.set_seed(seed)\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "class TrainingMonitor(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Monitors training metrics including loss trends and stability\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.batch_losses = []\n",
    "        self.loss_changes = []  # Track relative loss changes instead of gradients\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        # Store the batch loss\n",
    "        current_loss = logs['loss']\n",
    "        self.batch_losses.append(current_loss)\n",
    "        \n",
    "        # Calculate relative loss change (as a proxy for gradient behavior)\n",
    "        if len(self.batch_losses) > 1:\n",
    "            loss_change = abs((current_loss - self.batch_losses[-2]) / self.batch_losses[-2])\n",
    "            self.loss_changes.append(loss_change)\n",
    "        else:\n",
    "            self.loss_changes.append(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04922a6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 2: Training Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_params(learning_rate, momentum, use_nesterov=False):\n",
    "    \"\"\"Trains model with specific learning rate and momentum settings\"\"\"\n",
    "    model = create_model()\n",
    "    \n",
    "    # STUDENT TASK 3: Create SGD optimizer with given parameters\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    monitor = TrainingMonitor()\n",
    "    \n",
    "    # Train for a small number of epochs\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[monitor],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'history': history.history,\n",
    "        'batch_losses': monitor.batch_losses,\n",
    "        'loss_changes': monitor.loss_changes,\n",
    "        'final_loss': history.history['loss'][-1],\n",
    "        'final_accuracy': history.history['accuracy'][-1]\n",
    "    }\n",
    "\n",
    "def run_parameter_study(learning_rates, momentums):\n",
    "    \"\"\"Runs training with different combinations of learning rates and momentums\"\"\"\n",
    "    results = []\n",
    "    #     # STUDENT TASK 4: Create nested loops to test all combinations\n",
    "    #     # Include both standard momentum and Nesterov momentum\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311c961",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 3: Visualization Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmaps(results_df):\n",
    "    \"\"\"Creates heatmaps for loss and stability across parameter combinations\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "    # STUDENT TASK 5: Create four heatmaps\n",
    "    \n",
    "    # Standard momentum accuracy\n",
    "    \n",
    "    # Nesterov momentum accuracy\n",
    "    \n",
    "    # Standard momentum stability (using loss changes)\n",
    "    \n",
    "    # Nesterov momentum stability\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5d483",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 4: Running the Experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ec1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "momentums = [0.0, 0.5, 0.9, 0.99]\n",
    "\n",
    "# Run the study\n",
    "\n",
    "# Create visualizations\n",
    "\n",
    "# Print best configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadeb86",
   "metadata": {},
   "source": [
    "# Exercise 3: Adaptive Learning Rate Emergency\n",
    "\n",
    "In this exercise, you'll implement a custom callback that monitors training stability and automatically adjusts the learning rate when problems are detected. This represents a real-world scenario where you need to rescue training that's becoming unstable.\n",
    "\n",
    "## Setup and Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f9ad4a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load and preprocess Fashion MNIST\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3029c4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 1: Training Monitor Callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256628d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingEmergencyCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, \n",
    "                 patience=5,\n",
    "                 loss_spike_threshold=1.5,\n",
    "                 min_lr=1e-6):\n",
    "        super().__init__()\n",
    "        self.patience = patience\n",
    "        self.loss_spike_threshold = loss_spike_threshold\n",
    "        self.min_lr = min_lr\n",
    "        \n",
    "        self.loss_history = []\n",
    "        self.lr_history = []\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        # STUDENT TASK 1: Initialize monitoring variables\n",
    "        # Keep track of consecutive problems and best loss\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def check_training_problems(self, logs):\n",
    "        \"\"\"Checks for potential training problems\"\"\"\n",
    "        current_loss = logs['loss']\n",
    "        current_lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n",
    "        \n",
    "        # Store history\n",
    "        self.loss_history.append(current_loss)\n",
    "        self.lr_history.append(current_lr)\n",
    "        \n",
    "        problems = []\n",
    "        \n",
    "        # Check for loss spikes\n",
    "        if len(self.loss_history) > 1:\n",
    "            avg_previous_loss = np.mean(self.loss_history[-5:]) if len(self.loss_history) >= 5 else self.loss_history[-1]\n",
    "            if current_loss > avg_previous_loss * self.loss_spike_threshold:\n",
    "                problems.append(\"Loss spike detected\")\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        if np.isnan(current_loss) or np.isinf(current_loss):\n",
    "            problems.append(\"NaN/Inf values detected\")\n",
    "            \n",
    "        # Check for consistently increasing loss\n",
    "        if len(self.loss_history) >= 5:\n",
    "            if all(x < y for x, y in zip(self.loss_history[-5:], self.loss_history[-4:])):\n",
    "                problems.append(\"Consistently increasing loss\")\n",
    "                \n",
    "        return problems\n",
    "    \n",
    "    def adjust_learning_rate(self, problems):\n",
    "        \"\"\"Adjusts learning rate based on detected problems\"\"\"\n",
    "        current_lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n",
    "        \n",
    "\n",
    "        if problems:\n",
    "            # STUDENT TASK 4: Implement learning rate adjustment\n",
    "            # Reduce learning rate if there are problems\n",
    "            # Make sure new lr isn't below min_lr\n",
    "            pass\n",
    "        return False\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        problems = self.check_training_problems(logs)\n",
    "        \n",
    "        if problems:\n",
    "            self.consecutive_problems += 1\n",
    "            logger.info(f\"Training problems detected: {problems}\")\n",
    "            \n",
    "            if self.consecutive_problems >= self.patience:\n",
    "                logger.warning(f\"Exceeded patience - adjusting learning rate\")\n",
    "                self.adjust_learning_rate(problems)\n",
    "                self.consecutive_problems = 0\n",
    "        else:\n",
    "            self.consecutive_problems = 0\n",
    "            current_loss = logs['loss']\n",
    "            if current_loss < self.best_loss:\n",
    "                self.best_loss = current_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c40bc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 2: Training Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Creates a model prone to training instability\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def train_with_emergency_monitoring(initial_lr=0.1, epochs=10, patience=3):\n",
    "    \"\"\"Trains model with emergency monitoring\"\"\"\n",
    "    model = create_model()\n",
    "    \n",
    "    # STUDENT TASK 5: Create optimizer and compile model\n",
    "    \n",
    "    # Create callback\n",
    "    \n",
    "    # Train model\n",
    "    \n",
    "    \n",
    "    return history, emergency_cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4ef4c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 3: Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(callback):\n",
    "    \"\"\"Visualizes training metrics and learning rate adjustments\"\"\"\n",
    "    fig, (ax1, ax3) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    \n",
    "    # Plot loss history\n",
    "    ax1.plot(callback.loss_history)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Plot learning rate changes\n",
    "    ax3.plot(callback.lr_history)\n",
    "    ax3.set_title('Learning Rate')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Run training\n",
    "history, callback = train_with_emergency_monitoring(patience=5)\n",
    "plot_training_metrics(callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
