1. How do we determine a good learning rate?
2. What happens if your learning is too high?  Too low?
3. What is the key implementation difference between power scheduling and performance scheduling?
4. How does 1cycle scheduling work?
5. What does learning optimization do?
6. Provide an intuitive description of momentum optimization?
7. What does the Nesterov Accelerated Gradient do?
8. How does AdaGrad work?
9. What problem in AdaGrad is RMSProp designed to address?
10. How does Adam work?
11. What is the infinity-norm of a vector?
12. What optimization strategy should you use?  What if it doesn't work?


