{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b01507f6",
   "metadata": {
    "problem_id": "ex1"
   },
   "source": [
    "# Exercise 1: Implementing a Learning Rate Finder\n",
    "<!-- @q -->\n",
    "\n",
    "In this exercise, we'll implement a basic learning rate finder that helps identify good learning rates for training neural networks. The learning rate finder works by training the model for a few iterations while exponentially increasing the learning rate and monitoring the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6eaa36",
   "metadata": {
    "pass_through": true
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fdbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @SHOW\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess Fashion MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train = X_train_full.astype('float32') / 255.0\n",
    "y_train = y_train_full\n",
    "\n",
    "# Create a simple model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6a382",
   "metadata": {
    "part_id": "ex1-part1"
   },
   "source": [
    "## Part 1: Learning Rate Finder Callback\n",
    "\n",
    "\n",
    "First, we'll create a callback that increases the learning rate exponentially and records the loss at each step.  You just need to fill in the definition for the `self.lr_factor` variable, and then use it to update the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a536919",
   "metadata": {
    "part_id": "ex1-part1",
    "span": "ex1-part1.fill",
    "student": true
   },
   "outputs": [],
   "source": [
    "class LearningRateFinder(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, min_lr=1e-7, max_lr=10, steps=200):\n",
    "        super().__init__()\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.steps = steps\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "        \n",
    "        # Calculate the multiplication factor for each step\n",
    "        # STUDENT TASK: Calculate the factor that, when multiplied steps times,\n",
    "        # goes from min_lr to max_lr\n",
    "# TODO: Replace with your code (fill)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Start with minimum learning rate\n",
    "        #print(f\"Setting min rate {self.min_lr}\")\n",
    "        self.model.optimizer.learning_rate.assign(self.min_lr)\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "        # Get the current learning rate and loss\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        loss = logs['loss']\n",
    "        \n",
    "        # Store the values\n",
    "        self.learning_rates.append(tf.keras.backend.get_value(lr))\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        # Stop if the loss is not finite (exploding)\n",
    "        if not np.isfinite(loss):\n",
    "            print('Stopping - Loss is not finite!')\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "        \n",
    "        # Increase the learning rate for the next iteration\n",
    "        # STUDENT TASK: Update the learning rate using the lr_factor\n",
    "# TODO: Replace with your code (fill)\n",
    "        self.model.optimizer.learning_rate.assign(new_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019d7e1",
   "metadata": {
    "part_id": "ex1-part2"
   },
   "source": [
    "## Part 2: Finding the Learning Rate\n",
    "\n",
    "\n",
    "Now we'll create a function that uses our callback to find a good learning rate.  You'll need to fill the code for model compilation and fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c0aee8",
   "metadata": {
    "part_id": "ex1-part2",
    "span": "ex1-part2.fill",
    "student": true
   },
   "outputs": [],
   "source": [
    "def find_learning_rate(model, X, y, batch_size=64, steps=200):\n",
    "    # Initialize the learning rate finder\n",
    "    lr_finder = LearningRateFinder(steps=steps)\n",
    "    \n",
    "    # Compile the model\n",
    "    # STUDENT TASK: Compile the model with SGD optimizer and sparse_categorical_crossentropy\n",
    "# TODO: Replace with your code (fill)\n",
    "    \n",
    "    # Calculate the number of samples to use\n",
    "    num_samples = steps * batch_size\n",
    "    \n",
    "    # If we have more samples than we need, take a random subset\n",
    "    if len(X) > num_samples:\n",
    "        idx = np.random.randint(len(X), size=num_samples)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "    \n",
    "    # Train the model with the learning rate finder; make sure to include the `lr_finder` as a callback\n",
    "    # and use the batch size parameter that was passed into the function\n",
    "\n",
    "# TODO: Replace with your code (fill)\n",
    "    \n",
    "    return lr_finder.learning_rates, lr_finder.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbe470",
   "metadata": {
    "part_id": "ex1-part3"
   },
   "source": [
    "## Part 3: Visualizing and Analyzing Results\n",
    "\n",
    "\n",
    "Finally, we'll create a function to plot and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36633bc",
   "metadata": {
    "part_id": "ex1-part3",
    "span": "ex1-part3.fill",
    "student": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_rate(learning_rates, losses):\n",
    "    # Remove any infinite or nan losses\n",
    "    valid_idx = np.isfinite(losses)\n",
    "    learning_rates = np.array(learning_rates)[valid_idx]\n",
    "    losses = np.array(losses)[valid_idx]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(learning_rates, losses)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs. Learning Rate')\n",
    "    \n",
    "    # STUDENT TASK: Find the learning rate with minimum loss; store the learning rate in `best_lr`\n",
    "# TODO: Replace with your code (fill)\n",
    "\n",
    "    # Add a dot at the minimum loss\n",
    "    plt.plot(best_lr, losses[min_loss_idx], 'ro')\n",
    "    \n",
    "    # Add a text annotation\n",
    "    plt.annotate(f'Best LR: {best_lr:.2e}', \n",
    "                xy=(best_lr, losses[min_loss_idx]),\n",
    "                xytext=(best_lr*1.5, losses[min_loss_idx]*1.1),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_lr\n",
    "\n",
    "# Now let's run everything!\n",
    "model = create_model()\n",
    "lr_values, loss_values = find_learning_rate(model, X_train, y_train)\n",
    "print(f\"lr {len(lr_values)} loss {len(loss_values)}\")\n",
    "best_lr = plot_learning_rate(lr_values, loss_values)\n",
    "print(f\"\\nRecommended learning rate: {best_lr:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a059d3",
   "metadata": {
    "problem_id": "ex2"
   },
   "source": [
    "# Exercise 2: Optimizer Comparison with Noisy Data\n",
    "<!-- @q -->\n",
    "\n",
    "In this exercise, we'll explore how different optimizers perform when training on data with varying levels of noise. We'll learn how adaptive and non-adaptive optimizers respond differently to noisy gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0921d6ee",
   "metadata": {
    "pass_through": true
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @SHOW\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221b6ca",
   "metadata": {
    "part_id": "ex2-part1"
   },
   "source": [
    "## Part 1: Data Generation\n",
    "\n",
    "\n",
    "\n",
    "First, we'll create a function to generate synthetic data with controllable noise levels.  You just need to add the noise here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea76295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @SHOW\n",
    "def generate_noisy_data(n_samples=1000, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Generates synthetic classification data with controlled noise\n",
    "    and non-linear decision boundaries\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to generate\n",
    "        noise_level: Controls both cluster overlap and noise magnitude\n",
    "    \"\"\"\n",
    "    # Number of samples per class\n",
    "    n_per_class = n_samples // 4\n",
    "    \n",
    "    # Generate four overlapping clusters in a spiral pattern\n",
    "    t = np.linspace(0, 4*np.pi, n_per_class)\n",
    "    \n",
    "    # First two clusters (class 0)\n",
    "    r1 = 2 + 0.2 * noise_level * np.random.randn(n_per_class)\n",
    "    x1 = r1 * np.cos(t)\n",
    "    y1 = r1 * np.sin(t)\n",
    "    \n",
    "    r2 = 4 + 0.2 * noise_level * np.random.randn(n_per_class)\n",
    "    x2 = r2 * np.cos(t + np.pi/2)\n",
    "    y2 = r2 * np.sin(t + np.pi/2)\n",
    "    \n",
    "    # Second two clusters (class 1)\n",
    "    r3 = 3 + 0.2 * noise_level * np.random.randn(n_per_class)\n",
    "    x3 = r3 * np.cos(t + np.pi/4)\n",
    "    y3 = r3 * np.sin(t + np.pi/4)\n",
    "    \n",
    "    r4 = 5 + 0.2 * noise_level * np.random.randn(n_per_class)\n",
    "    x4 = r4 * np.cos(t + 3*np.pi/4)\n",
    "    y4 = r4 * np.sin(t + 3*np.pi/4)\n",
    "    \n",
    "    # Combine all clusters\n",
    "    X = np.vstack([\n",
    "        np.column_stack([x1, y1]),\n",
    "        np.column_stack([x2, y2]),\n",
    "        np.column_stack([x3, y3]),\n",
    "        np.column_stack([x4, y4])\n",
    "    ])\n",
    "    \n",
    "    # Create labels\n",
    "    y = np.hstack([\n",
    "        np.zeros(2*n_per_class),\n",
    "        np.ones(2*n_per_class)\n",
    "    ])\n",
    "    \n",
    "    # STUDENT TASK\n",
    "    # Add random noise to the data set to make it a little more challenging\n",
    "    # @FILL begin\n",
    "    X += noise_level * np.random.randn(*X.shape)\n",
    "    # @FILL end\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    idx = np.random.permutation(len(X))\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4abf4",
   "metadata": {
    "part_id": "ex2-part2"
   },
   "source": [
    "## Part 2: Model Creation\n",
    "\n",
    "\n",
    "The following just sets up some code to create the model and optimizer.  You need to complete the `get_optimizer` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25bf0d",
   "metadata": {
    "part_id": "ex2-part2",
    "span": "ex2-part2.fill",
    "student": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Creates a simple neural network classifier\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(16, activation='relu', input_shape=(2,)),\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def get_optimizer(optimizer_name, learning_rate):\n",
    "    \"\"\"\n",
    "    Creates an optimizer instance based on name and learning rate\n",
    "    \n",
    "    Args:\n",
    "        optimizer_name: String name of optimizer ('sgd', 'adam', etc.)\n",
    "        learning_rate: Learning rate to use\n",
    "    \"\"\"\n",
    "    # STUDENT TASK: Create and return the appropriate optimizer\n",
    "    # Include momentum=0.9 for SGD\n",
    "# TODO: Replace with your code (fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b5142c",
   "metadata": {
    "part_id": "ex2-part3"
   },
   "source": [
    "## Part 3: Training and Evaluation\n",
    "\n",
    "Now we'll develop code to iterate over our experimental parameters.  You just need to implement the code to sweep the parameter space (optimizers, learning rates, noise levels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91778fa1",
   "metadata": {
    "part_id": "ex2-part3",
    "span": "ex2-part3.fill",
    "student": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(X, y, optimizer_name, learning_rate, noise_level, \n",
    "                      epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains a model and returns training history\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = create_model()\n",
    "    optimizer = get_optimizer(optimizer_name, learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(X_train, y_train,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       validation_data=(X_test, y_test),\n",
    "                       verbose=0)\n",
    "    \n",
    "    # Get final test accuracy\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    return history.history, test_acc\n",
    "\n",
    "def compare_optimizers(noise_levels=[0.1, 0.5, 1.0], \n",
    "                      learning_rates=[0.001, 0.01, 0.1]):\n",
    "    \"\"\"\n",
    "    Compares optimizers across different noise levels and learning rates\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    optimizers = ['SGD', 'Adam', 'RMSprop']\n",
    "    \n",
    "    # STUDENT TASK: Create nested loops to test all combinations\n",
    "    # Loop through noise levels, optimizers, and learning rates\n",
    "# TODO: Replace with your code (fill)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c632d409",
   "metadata": {
    "part_id": "ex2-part4"
   },
   "source": [
    "## Part 4: Visualization\n",
    "\n",
    "Write some code to help visualize the results.  You'll need to construct 3 heatmaps (one for each optimizer) illustrating performance for the 9 different parameter combinations (each heatmap should have 9 cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6faa9",
   "metadata": {
    "part_id": "ex2-part4",
    "span": "ex2-part4.fill",
    "student": true
   },
   "outputs": [],
   "source": [
    "def plot_results(results_df):\n",
    "    \"\"\"\n",
    "    Creates visualizations of the results\n",
    "    \"\"\"\n",
    "    # Create a heatmap for each optimizer\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, optimizer in enumerate(['SGD', 'Adam', 'RMSprop']):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        \n",
    "        # STUDENT TASK: Pivot the data and create a heatmap (use SNS)\n",
    "        # Rows should be noise levels, columns should be learning rates\n",
    "# TODO: Replace with your code (fill)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the experiment\n",
    "results = compare_optimizers()\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f71df",
   "metadata": {
    "part_id": "ex2-part5"
   },
   "source": [
    "## Questions:\n",
    "\n",
    "\n",
    "1. Why do adaptive optimizers (Adam, RMSprop) typically perform better with noisy data?\n",
    "2. How does the optimal learning rate change with noise level for each optimizer?\n",
    "3. What happens if we increase the number of training epochs? Does it affect different optimizers differently?\n",
    "4. How would you modify this experiment to test the optimizers' robustness to different types of noise (e.g., outliers vs. Gaussian noise)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d9d9a",
   "metadata": {
    "part_id": "ex2-part5",
    "span": "ex2-part5.answer",
    "student": true
   },
   "source": [
    "*Enter your answer in this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4b9c8",
   "metadata": {
    "problem_id": "ex3"
   },
   "source": [
    "# Exercise 3: Custom Learning Rate Schedule\n",
    "<!-- @q -->\n",
    "\n",
    "In this exercise, we'll create a custom learning rate schedule that combines multiple scheduling strategies. Our schedule will implement a \"warm-up\" period, followed by exponential decay, and include periodic \"restarts\" where the learning rate temporarily increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845066c7",
   "metadata": {
    "pass_through": true
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @SHOW\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "# Take 5000 samples\n",
    "n_samples = 5000\n",
    "indices = np.random.permutation(len(X_train_full))[:n_samples]\n",
    "X_train = X_train_full[indices] / 255.0\n",
    "y_train = y_train_full[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd29a92",
   "metadata": {
    "part_id": "ex3-part1"
   },
   "source": [
    "## Part 1: Custom Learning Rate Schedule Implementation\n",
    "\n",
    "\n",
    "Here, we're going to create a class which, after an initial warmup period, cycles the learning rate using a cosine function with a periodic restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62305b7",
   "metadata": {
    "part_id": "ex3-part1",
    "span": "ex3-part1.fill",
    "student": true
   },
   "outputs": [],
   "source": [
    "class WarmupCosineRestart(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Custom learning rate schedule with:\n",
    "    1. Linear warmup period\n",
    "    2. Cosine decay - each cycle should use cosine to reduce the LR from \"initial_learning_rate\" to zero\n",
    "    3. Regular restarts - at the end of the each cycle, the LR should be reset to the \"initial_learning_rate\"\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, warmup_steps, decay_steps, alpha=0.0):\n",
    "        super(WarmupCosineRestart, self).__init__()\n",
    "        \n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.alpha = alpha  # Minimum learning rate factor\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        # Convert step to float32\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        \n",
    "        # STUDENT TASK 1: Implement warmup phase\n",
    "        # During warmup, LR should increase linearly from 0 to initial_learning_rate\n",
    "        \n",
    "# TODO: Replace with your code (fill)\n",
    "\n",
    "        # STUDENT TASK 2: Implement cosine decay with restart\n",
    "        # Calculate the current cycle and progress within cycle\n",
    "        \n",
    "# TODO: Replace with your code (fill)\n",
    "\n",
    "\n",
    "        # Combine warmup and decay\n",
    "        # You'll need to have warmup_lr and cosine_decay (varying from 0 to 1) defined above\n",
    "        lr = tf.cond(\n",
    "            step < self.warmup_steps,\n",
    "            lambda: warmup_lr,\n",
    "            lambda: self.initial_learning_rate * cosine_decay\n",
    "        )\n",
    "        \n",
    "        return lr\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_learning_rate\": self.initial_learning_rate,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "            \"alpha\": self.alpha\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe0ef60",
   "metadata": {
    "part_id": "ex3-part2"
   },
   "source": [
    "## Part 2: Training Loop and Visualization\n",
    "\n",
    "Use the following code cell to create the training loop and visualize learning rates.  Create and contrast three different learning schedules:\n",
    "\n",
    "- An exponential decay LR (initial learning rate == .001, decay_rate = .9, decay = 2 * steps_per_epoch)\n",
    "- A demonstration of 'warmup_cosine' with one set of parameters\n",
    "- Another demonstration of 'warmup_cosine' with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0090197c",
   "metadata": {
    "part_id": "ex3-part2",
    "span": "ex3-part2.fill",
    "student": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Creates a simple CNN model\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def plot_schedule(schedule, steps):\n",
    "    \"\"\"Plots the learning rate schedule\"\"\"\n",
    "    lrs = [schedule(step).numpy() for step in range(steps)]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(lrs)\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def train_and_compare_schedules(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains models with different learning rate schedules and compares them\"\"\"\n",
    "    \n",
    "    # Calculate steps per epoch\n",
    "    batch_size = 32\n",
    "    steps_per_epoch = len(X_train) // batch_size\n",
    "    \n",
    "    # STUDENT TASK 3: Create three different schedule configurations\n",
    "# TODO: Replace with your code (fill)\n",
    "    \n",
    "    histories = {}\n",
    "    \n",
    "    # Train with each schedule\n",
    "    for name, schedule in schedules.items():\n",
    "        # STUDENT TASK 4: Create and compile model\n",
    "# TODO: Replace with your code (fill)\n",
    "        \n",
    "        # Reshape data for CNN\n",
    "        X_train_reshaped = X_train.reshape(-1, 28, 28, 1)\n",
    "        X_test_reshaped = X_test.reshape(-1, 28, 28, 1)\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_reshaped, y_train,\n",
    "            epochs=10,\n",
    "            validation_data=(X_test_reshaped, y_test),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        histories[name] = history.history\n",
    "    \n",
    "    return histories\n",
    "\n",
    "def visualize_results(histories):\n",
    "    \"\"\"Plots training curves for different schedules\"\"\"\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, history in histories.items():\n",
    "        plt.plot(history['loss'], label=name)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, history in histories.items():\n",
    "        plt.plot(history['accuracy'], label=name)\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27f66d",
   "metadata": {
    "part_id": "ex3-part3"
   },
   "source": [
    "## Part 3: Running the Experiment\n",
    "\n",
    "Run your experiment.  Make sure to plot at least one of your WarmupCosineRestart schedules to make sure the implementation does what you think it will.  Then run the training and comparison code to compare all three schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e176ab",
   "metadata": {
    "part_id": "ex3-part3",
    "span": "ex3-part3.code",
    "student": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6c60e",
   "metadata": {
    "part_id": "ex3-part4"
   },
   "source": [
    "## Questions:\n",
    "\n",
    "\n",
    "\n",
    "1. How does the warmup period affect the early stages of training?\n",
    "2. What are the advantages and disadvantages of learning rate restarts?\n",
    "3. How would you modify the schedule for a very deep network?\n",
    "4. What considerations would you make when choosing the warmup_steps and decay_steps parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c9a8e",
   "metadata": {
    "part_id": "ex3-part4",
    "span": "ex3-part4.answer",
    "student": true
   },
   "source": [
    "*Enter your answer in this cell*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
